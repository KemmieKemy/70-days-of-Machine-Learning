# 70 days of Machine Learning
_This repository was made to keep track of my progress during Data Science Nigeria's 70 days of Machine Learning_<br>
* Day 1 - Introduction to Machine Learning
* Day 7 - How Linear Regression works
* <b> Day 10 - R-Squared theory;</b> <br> 
Another name for R-squared error is Coefficient of Determination. <br>
Error is the distance between a point and a line of best fit. <br>
The error is squared to get a positive value of our error.
* <b> Day 13 - Introduction to K Nearest Neighbors algorithm; </b> <br>
'K' in the "K Nearest Neighbors" algorithm is a parameter that refers to the number of nearest neighbors to consider during voting process. <br>
K Nearest Neighbors algorithm is classified as supervised learning. <br>
Clustering is the process of dividing data points into a number of similar groups. <br>
* <b> Day 21 - Understanding Vectors; </b> <br>
The magnitude of a vector is denoted with Bars. <br>
Learnt how to calculate the magnitude of the vector. <br>
* <b> Day 22 - Support Vector Assertion; </b> <br>
Dot product is the relationship between the input and weight. <br>
If vector "u", dotted with vector "w + b" equals zero, it means that Vector u is on the decision boundary. <br>
If vector "u", dotted with vector "w + b" is greater or equal to zero, it means that the sample is of a class above the hyperplane. <br>
* <b> Day 23 - Support Vector Machine Fundamentals; </b> <br>
A support vector is a feature set that if moved, affects the position of the best separating hyperplane. <br>
* <b> Day 24 - Support Vector Machine Optimization; </b> <br>
Equation for hyperplane is X.W + b <br>
Support Vector Machines are less effective when the data is noisy and contains overlapping points. <br>
* <b> Day 29 & 30 - Introduction to Kernels; </b> <br>
Kernels are done using inner product. <br>
Kernels take two inputs and outputs the similarities. <br>
Inner Product is a projection of x1 onto x2. <br>
Kernel is represented using the greek letter "phi". <br>
Transformation of the old and creation of new hyperplane helps SVM to perform better on non-linearly separable data. <br>
The default kernel for SVM using sckit-learn is Radio base function. <br>
* <b> Day 31 - Soft Margin SVM; </b> <br>
For a more generalised model, the best kernel that represents the dataset has to be found. <br>
Soft Margin Classifier is a classifier with violating data in the separating hyperplane. <br>
Hard Margin Classifier is a classifier having perfectly separated data points in the decision hyperplane.<br>
In Soft margin there's a degree of error called Slack.<br>
The value of slack in SVM can best be represented with S>=0 .<br>
A slack value of Zero indicates a Hard Margin. <br>
Given that (SV(support Vectors) / Number of samples ) > 1, indicates; Overfitting, Non- linearly separable data and Wrong kernel.<br>
* <b> Day 33 - Support Vector Machine Parameters </b> <br>
SVM is a binary classifier, so it can only seperate two groups per decision boundary; OVO - One versus One and OVR - One Versus Rest. <br>
I learnt about a lot of SVM Parameters. <br>
* <b> Day 43 - Introduction to Neural Networks. </b> <br>
A neural network with two or more hidden layers. <br>
The layers of a neural network are; Input, Hidden and Output. <br>
An activation function serves as a threshold for determining output value. <br>
The idea of a Neural Network is dea is to mimic a neuron, and, with a basic neuron, you have the dendrites, a nucleus, axon, and terminal axon. <br>
* <b> Day 44 - Installation of TensorFlow. </b> <br>
<br>



```diff
+ #DSN70daysofML
```
